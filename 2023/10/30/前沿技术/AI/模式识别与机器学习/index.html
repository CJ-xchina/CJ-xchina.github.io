<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>模式识别与机器学习 | Ther的小站</title><meta name="author" content="Ther"><meta name="copyright" content="Ther"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="概述&#x3D;&#x3D;模式&#x3D;&#x3D;：在于时间和空间可观测的物体，可以区别其是否相同或相似。&#x3D;&#x3D;模式的直观特性&#x3D;&#x3D;：  可观测性 可区分性 相似性  &#x3D;&#x3D;模式识别与机器学习的目的&#x3D;&#x3D;：利用计算机对对象进行分类，在错误概率最小的条件下，使识别的结果与客观物体相符合，即： Y &#x3D; F(X)">
<meta property="og:type" content="article">
<meta property="og:title" content="模式识别与机器学习">
<meta property="og:url" content="http://cjx_0723.gitee.io/2023/10/30/%E5%89%8D%E6%B2%BF%E6%8A%80%E6%9C%AF/AI/%E6%A8%A1%E5%BC%8F%E8%AF%86%E5%88%AB%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/index.html">
<meta property="og:site_name" content="Ther的小站">
<meta property="og:description" content="概述&#x3D;&#x3D;模式&#x3D;&#x3D;：在于时间和空间可观测的物体，可以区别其是否相同或相似。&#x3D;&#x3D;模式的直观特性&#x3D;&#x3D;：  可观测性 可区分性 相似性  &#x3D;&#x3D;模式识别与机器学习的目的&#x3D;&#x3D;：利用计算机对对象进行分类，在错误概率最小的条件下，使识别的结果与客观物体相符合，即： Y &#x3D; F(X)">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://i1.hdslb.com/bfs/face/b6bef67834c33bc7d386fb03a6dbc6db7966102e.jpg@240w_240h_1c_1s_!web-avatar-nav.avif">
<meta property="article:published_time" content="2023-10-30T14:04:29.021Z">
<meta property="article:modified_time" content="2023-10-30T14:04:29.021Z">
<meta property="article:author" content="Ther">
<meta property="article:tag" content="AI">
<meta property="article:tag" content="课堂学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://i1.hdslb.com/bfs/face/b6bef67834c33bc7d386fb03a6dbc6db7966102e.jpg@240w_240h_1c_1s_!web-avatar-nav.avif"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://cjx_0723.gitee.io/2023/10/30/%E5%89%8D%E6%B2%BF%E6%8A%80%E6%9C%AF/AI/%E6%A8%A1%E5%BC%8F%E8%AF%86%E5%88%AB%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><meta/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: {"appId":"YGMR4LH4DH","apiKey":"1fb48f3686d76eaf600890b7d3bb69c9","indexName":"index_hexo","hits":{"per_page":8},"languages":{"input_placeholder":"搜索文章","hits_empty":"找不到您查询的内容：${query}","hits_stats":"找到 ${hits} 条结果，用时 ${time} 毫秒"}},
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"你已切换为繁体中文","cht_to_chs":"你已切换为简体中文","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#49b1f5","bgDark":"#2d3035","position":"bottom-left"},
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '模式识别与机器学习',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2023-10-30 22:04:29'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.3.0"><link rel="alternate" href="/atom.xml" title="Ther的小站" type="application/atom+xml">
</head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://i1.hdslb.com/bfs/face/b6bef67834c33bc7d386fb03a6dbc6db7966102e.jpg@240w_240h_1c_1s_!web-avatar-nav.avif" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">53</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">29</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">8</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-heartbeat"></i><span> 清单</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/Gallery/"><i class="fa-fw fas fa-images"></i><span> 照片</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://w.wallhaven.cc/full/yj/wallhaven-yjvxeg.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="Ther的小站"><span class="site-name">Ther的小站</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-heartbeat"></i><span> 清单</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/Gallery/"><i class="fa-fw fas fa-images"></i><span> 照片</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">模式识别与机器学习</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-10-30T14:04:29.021Z" title="发表于 2023-10-30 22:04:29">2023-10-30</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-10-30T14:04:29.021Z" title="更新于 2023-10-30 22:04:29">2023-10-30</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E5%89%8D%E6%B2%BF%E6%8A%80%E6%9C%AF/">前沿技术</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">7.5k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>24分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="模式识别与机器学习"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><meta name="referrer" content="no-referrer"/>

<h1 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h1><p>&#x3D;&#x3D;模式&#x3D;&#x3D;：在于时间和空间可观测的物体，可以区别其是否相同或相似。<br>&#x3D;&#x3D;模式的直观特性&#x3D;&#x3D;：</p>
<ul>
<li>可观测性</li>
<li>可区分性</li>
<li>相似性</li>
</ul>
<p>&#x3D;&#x3D;模式识别与机器学习的目的&#x3D;&#x3D;：利用计算机对对象进行分类，在错误概率最小的条件下，使识别的结果与客观物体相符合，即： Y &#x3D; F(X) ，X为输入特征集，Y为机器识别类别的标号集，F是模式识别判定算法。<br>得到F的方法，就是通过大量训练数据学习，提升F预测结果。</p>
<p>&#x3D;&#x3D;机器学习的概念&#x3D;&#x3D;：研究如何构造理论、算法和计算机系统，让机器通过数据中学习后进行分类、识别事物、推理预测和预测未来等。</p>
<p>&#x3D;&#x3D;模式识别系统目标&#x3D;&#x3D;：在特征空间与解释空间之间找到一种映射关系，该关系成为假设。</p>
<ul>
<li>特征空间：从模式得到的对分类有用的度量、属性或基元构成的空间。</li>
<li>解释空间：所属类别的集合。</li>
</ul>
<p>获得假说的两种方式：</p>
<ol>
<li>监督学习、概念驱动或归纳假说</li>
<li>非监督学习、数据驱动或演绎假说</li>
</ol>
<h1 id="第二章、生成式分类器"><a href="#第二章、生成式分类器" class="headerlink" title="第二章、生成式分类器"></a>第二章、生成式分类器</h1><h2 id="2-1-贝叶斯判别准则"><a href="#2-1-贝叶斯判别准则" class="headerlink" title="2.1 贝叶斯判别准则"></a>2.1 贝叶斯判别准则</h2><p>在给定观测值$x$的情况下，判断其属于$w_1$类还是$w_2$，作出某次判断时的错误率是：<br>$$<br>P(error|x)&#x3D;\begin{cases}P(\omega_{1}|x),x\in\omega_{2}\P(\omega_{2}|x),x\in\omega_{1}\end{cases}<br>$$<br>在贝叶斯判别准则下，若$P( \omega _ {1}  |x)&gt;P(  \omega _ {2}  |x)$ 则判定类别$x\in w_1$，否则判别$x\in w_2$ ，该判别准则也就是要确定$x$是属于哪一类，这要取决于$x$来自哪一类的概率更大，即：</p>
<img src="https://typora-md-bucket.oss-cn-beijing.aliyuncs.com/image-20230919173027970.png" alt="image-20230919173027970" style="zoom: 67%;" />

<p>回顾一下<strong>全概率公式</strong>，设样本空间有随机事件$w_1，w_2，w_3，….w_n$，这$n$个随机事件两两不相容，即$w_1w_2&#x3D;\phi$，且满足$\bigcup_{i&#x3D;1}^{n}w_i&#x3D;\Omega$，设$x$为$\Omega$中的一个事件，则全概率公式为：<br>$$<br>P(x) &#x3D; \sum_{i&#x3D;1}^{n}P(x|w_i)P(w_i)<br>$$<br>回顾一下贝叶斯公式，样本空间有随机事件$w_1，w_2，w_3，….w_n$，这$n$个随机事件两两不相容，即$w_1w_2&#x3D;\phi$，且满足$\bigcup_{i&#x3D;1}^{n}w_i&#x3D;\Omega$，设$x$为$\Omega$中的一个事件，贝叶斯公式为：<br>$$<br>P(w_i|x)&#x3D;\frac{P(x|w_i)P(w_i)}{\sum_{j&#x3D;1}^{n}P(x|w_j)P(w_j)}&#x3D;\frac{P(x|w_i)P(w_i)}{P(x)}<br>$$<br>根据贝叶斯公式：$P(  \omega _ {1}  |x) &#x3D; \frac{P(x|w_1)P(w_1)}{P(x)}$  $P(  \omega _ {2}  |x) &#x3D; \frac{P(x|w_2)P(w_2)}{P(x)}$，而两项中$P(x)$均为全概率公式相等，因此比贝叶斯判别式为：</p>
<img src="https://typora-md-bucket.oss-cn-beijing.aliyuncs.com/image-20230919180132020.png" alt="image-20230919180132020" style="zoom: 50%;" />

<p>化为：</p>
<img src="https://typora-md-bucket.oss-cn-beijing.aliyuncs.com/image-20230919180553440.png" alt="image-20230919180553440" style="zoom:67%;" />

<p>其中$P(x|w_i)$为<strong>似然函数</strong> ，表示特征向量x来自类别$ω_i$的概率，图中还有似然比与判决阈值的定义。</p>
<h2 id="2-2-贝叶斯最小风险判别"><a href="#2-2-贝叶斯最小风险判别" class="headerlink" title="2.2 贝叶斯最小风险判别"></a>2.2 贝叶斯最小风险判别</h2><h3 id="代价矩阵"><a href="#代价矩阵" class="headerlink" title="代价矩阵"></a>代价矩阵</h3><p>如果分类器判别$x$是属于$w_j$类，但它实际上来自$w_i$类，也就是说分类器判断失败，错误的分类会产生’损失’或’代价’，将代价存储在一个矩阵$L_{i，j}$中，$L_{i，j}$代表分类器判定为$w_j$类其实其属于$w_i$类这种情况所产生的代价，更具体来说:</p>
<ul>
<li>$L_{i，j}$是个矩阵，其中$i$表示样本的真实类，$j$表示分类器预测的类。</li>
<li>当$i&#x3D;j$时，分类正确，$L_{i，j}$取 0 或负值，表示没有损失。</li>
<li>当$i≠j$时，分类错误$L_{i，j}$取正值，其数值表示错误分类的程度。</li>
</ul>
<p>例如:</p>
<ul>
<li>对数字识别问题来说，将5错误分类为3的损失可能比将5错误分类为0的损失小。</li>
<li>对病人诊断问题来说，将癌症患者错误判断为普通感冒的损失会比相反情况要大。</li>
</ul>
<p>所以$L_{i，j}$可以对应不同类型的错误设置不同的“代价值”，从而形成一个“损失矩阵”。</p>
<p>通过最小平均条件风险进行分类，实际上是在考虑各种错误的$L_{i，j}$值，选择分类误差最小的方案。这样可以有效减轻高风险错误的影响。</p>
<p>所以简单来说，$L_{i，j}$反映了分类器在不同类型分类错误下的“损失级别”，是贝叶斯最小风险分类的一个重要参数。</p>
<hr>
<h3 id="平均条件风险"><a href="#平均条件风险" class="headerlink" title="平均条件风险"></a>平均条件风险</h3><p>**平均条件风险(average conditional risk)**：样本$x$共有$M$种可供选择的类别，其所属真实类为$w_j$，将其分类到类$w_i$时的平均风险，也即：<br>$$<br>r_j&#x3D;  \sum _ {i&#x3D;1}^ {M} L_ {ij}  P(  w_{i}|x) &#x3D; \frac{\sum^{M}<em>{i&#x3D;1}L</em>{ij}P(x|w_i)P(w_i)}{P(x)}<br>$$<br>换句话说，平均条件风险考虑了:</p>
<ol>
<li>各种错误分类的损失程度($L_{ij}$表示)</li>
<li>不同错误发生的概率($P(w_i|x)$表示错误分类到$w_i$的概率)</li>
<li>对所有可能错误进行加权求和</li>
</ol>
<p>通常来说，最小平均条件风险分类器就是：对每个样本$x$计算所有类的平均条件风险$r_j，$将样本$x$指定为平均条件风险最小的那一个类。</p>
<h3 id="例题解析"><a href="#例题解析" class="headerlink" title="例题解析"></a>例题解析</h3><p>题目：</p>
<p><img src="https://typora-md-bucket.oss-cn-beijing.aliyuncs.com/image-20230919183559815.png" alt="image-20230919183559815"></p>
<ul>
<li>信道输入信号只可能为0或1。</li>
<li>信道传输过程中会加入高斯噪声。噪声的均值是0,方差是$σ^2$。</li>
<li>信道输出为x。</li>
<li>**高斯分布(Gaussian Distribution)**也称正态分布</li>
</ul>
<p>解析：</p>
<h4 id="方法一（平均条件风险）："><a href="#方法一（平均条件风险）：" class="headerlink" title="方法一（平均条件风险）："></a><strong>方法一（平均条件风险）：</strong></h4><p>噪声是服从$N(0, σ^2)$高斯分布，输入为0时,输出x ~ $N(0, σ^2)$;输入为1时,输出x ~$ N(1, σ^2)$</p>
<p>定义：**$w_1$：输出为0<strong>；</strong>$w_2$：输出为1<strong>。用贝叶斯判别条件分析：</strong>设信号送$0$的先验概率为**<br><strong>$P(0)$,送$1$的先验概率为$P(1)$</strong>,$L_{i，j}$的取值为：<img src="https://typora-md-bucket.oss-cn-beijing.aliyuncs.com/image-20230919184226689.png" alt="image-20230919184226689" style="zoom:33%;" /></p>
<p>当输入0或1时，x的值的概率分布为：<br>$$<br>p(x\mid\omega_{1})&#x3D;{\frac{1}{\sqrt{2\pi},\sigma}},e^{-\frac{x^{2}}{2\sigma^{2}}}\qquad P(x\mid\omega_{2})&#x3D;{\frac{1}{\sqrt{2\pi},\sigma}}e^{-\frac{(x-1)^{2}}{2\sigma^{2}}}<br>$$<br>写出两类的平均风险，并比较：<br>$$<br>\frac{r_{1}(x)}{r_{2}(x)}&#x3D;\frac{L_{11}p(x|\omega_{1})P(0)+L_{12}p(x|\omega_{2})P(1)}{L_{21}p(x|\omega_{1})P(0)+L_{22}p(x|\omega_{2})P(1)}<br>$$<br>如果$\frac{r_{1}(x)}{r_{2}(x)}&lt;1$，意味着预测为$w_1$的风险更小，结果为$w_1$，输出$0$信号。反之则输出$1$信号。</p>
<h4 id="方法二（似然比与阈值）："><a href="#方法二（似然比与阈值）：" class="headerlink" title="方法二（似然比与阈值）："></a>方法二（似然比与阈值）：</h4><p>还是根据方法一的思路：</p>
<p>$r_1(x)&#x3D;L_{11}p(x|\omega_{1})P(0)+L_{12}p(x|\omega_{2})P(1)$</p>
<p>$r_2(x)&#x3D;L_{21}p(x|\omega_{1})P(0)+L_{22}p(x|\omega_{2})P(1)$</p>
<p>因此有：</p>
<p>$r_1(x)-r_2(x) &#x3D; (L_{11}-L_{21})p(x|\omega_{1})P(0)+(L_{12}-L_{22})p(x|\omega_{2})P(1)$</p>
<p>可以得到：</p>
<img src="https://typora-md-bucket.oss-cn-beijing.aliyuncs.com/image-20230919185908144.png" alt="image-20230919185908144" style="zoom:50%;" />

<img src="https://typora-md-bucket.oss-cn-beijing.aliyuncs.com/image-20230919185921027.png" alt="image-20230919185921027" style="zoom: 50%;" />

















<h1 id="第三章、判别函数"><a href="#第三章、判别函数" class="headerlink" title="第三章、判别函数"></a>第三章、判别函数</h1><h2 id="3-1-判别式分类器与生成式分类器"><a href="#3-1-判别式分类器与生成式分类器" class="headerlink" title="3.1 判别式分类器与生成式分类器"></a>3.1 判别式分类器与生成式分类器</h2><p><img src="https://cdn.nlark.com/yuque/0/2023/png/22608736/1695709751405-97d91630-58b7-41b5-89e4-fb31a7320b98.png" alt="img"></p>
<p>判别式分类器：只关心情况的可能性，适合于绝大多数的情况。</p>
<p>生成式分类器：能够反映同类数据本身的相似度，但是不关心各分类的边界，相较于判别式分类器能够更快收敛。生成式模型能够应付存在隐变量的情况。</p>
<h2 id="3-2-线性判别函数"><a href="#3-2-线性判别函数" class="headerlink" title="3.2 线性判别函数"></a>3.2 线性判别函数</h2><p>略…</p>
<h2 id="3-3-广义线性判别函数"><a href="#3-3-广义线性判别函数" class="headerlink" title="3.3 广义线性判别函数"></a>3.3 广义线性判别函数</h2><p>线性判别函数实现简单但是无法处理复杂情况，而非线性判别函数能够处理复杂情况但是实现较困难。若能<strong>将非线性判别函数转换为线性判别函数</strong>则有利于模式分类的实现。</p>
<h3 id="核心思想"><a href="#核心思想" class="headerlink" title="核心思想"></a>核心思想</h3><p>训练集${x}$ 在模型空间$x$中线性不可分，但是在更高维度模式空间$x*$中线性可分，x<em>的维数k高于x的维数n，择取：<br>$$<br>x</em>&#x3D;(f_1(x),f_2(x),f_3(x),….,f_k(x)),k&gt;n<br>$$<br>此时只要将模式x进行非线性变换，使之变换后得到维数更高的模式$x *$ ，就可以用线性判别函数来进行分类。</p>
<h3 id="描述"><a href="#描述" class="headerlink" title="描述"></a>描述</h3><p><img src="https://typora-md-bucket.oss-cn-beijing.aliyuncs.com/image-20230928104312886.png" alt="image-20230928104312886"></p>
<ul>
<li>对于二次多项式情况：$d(x)&#x3D;w_{11}x_{1}^{2}+w_{12}x_{1}x_{2}+w_{22}x_{2}^{2}+w_{1}x_{1}+w_{2}x_{2}+w_{3}$</li>
</ul>
<p>​		   此时<img src="https://typora-md-bucket.oss-cn-beijing.aliyuncs.com/image-20230928104830062.png" alt="image-20230928104830062" style="zoom:67%;" /></p>
<ul>
<li>对于n维的情况：$d(x)&#x3D;\sum_{j&#x3D;1}^{n}w_{j j}x_{j}^{2}+\sum_{j&#x3D;1}^{n-1}\sum_{k&#x3D;j+1}^{n}w_{j k}x_{j}x_{k}+\sum_{j&#x3D;1}^{n}w_{j}x_{j}+w_{n+1}$</li>
</ul>
<p>​			此时总项数维：$n+n(n-1)&#x2F;2+n+1 &#x3D; (n+1)(n+2)&#x2F;2&gt;n$</p>
<p>​			此时x*各分量一般化形式： $f_{i}(x)&#x3D;x_{p_{1}}^{s}x_{p_{2}}^{t},p_{1},p_{2}&#x3D;1,2,\cdots,n;;s,t&#x3D;0,1$</p>
<h3 id="广义线性判别函数的意义"><a href="#广义线性判别函数的意义" class="headerlink" title="广义线性判别函数的意义"></a>广义线性判别函数的意义</h3><p>意义在于通过<strong>非线性变换</strong>将输入模式映射到一个高维特征空间中，使得在新的特征空间中模式更容易被线性判别函数分割。这种变换可以将原始的模式空间转换为一个更具可分性的特征空间，从而提高模式分类的准确性。</p>
<p>就如下面这个例子：</p>
<img src="https://typora-md-bucket.oss-cn-beijing.aliyuncs.com/image-20230928105547246.png" alt="image-20230928105547246" style="zoom:80%;" />

<p>原式：$d(x)&#x3D;w^Tx$ </p>
<p>现在令：$x*&#x3D;(x^2,x,1)^T&#x3D;(x_1,x_2,1)$ $w &#x3D; (1,-(a+b),ab)$</p>
<img src="https://typora-md-bucket.oss-cn-beijing.aliyuncs.com/image-20230928110008403.png" alt="image-20230928110008403" style="zoom:67%;" />

<h2 id="3-4-分段线性函数"><a href="#3-4-分段线性函数" class="headerlink" title="3.4 分段线性函数"></a>3.4 分段线性函数</h2><h3 id="出发点"><a href="#出发点" class="headerlink" title="出发点"></a>出发点</h3><p>线性判别函数简单有效但是无法适用于所有分类情况；广义线性判别函数能通过增大维度来得到线性判别，但是维度大量增加会导致计算复杂性增大。</p>
<p>因此引入分段线性判别函数的判别过程，<strong>它比一般的线性判别函数的错误率小，但又比非线性判别函数简单</strong></p>
<p>例如下面这个例子：</p>
<p><img src="https://typora-md-bucket.oss-cn-beijing.aliyuncs.com/image-20230928110348254.png" alt="image-20230928110348254"></p>
<h3 id="设计"><a href="#设计" class="headerlink" title="设计"></a>设计</h3><p>设计分段线性判别函数的方式为：<strong>最小距离分类法</strong></p>
<p>$u_1$和$u_2$为两类$w_1$和$w_2$的聚类中心（它代表着聚类中的一个代表性点或中心点，该点被认为是与同一聚类中的其他数据点最为接近的点），定义决策规则为：</p>
<img src="https://typora-md-bucket.oss-cn-beijing.aliyuncs.com/image-20230928111339081.png" alt="image-20230928111339081" style="zoom:67%;" />

<p>$||x-u_i||^2$  表示随机变量$x$到$u_i$的距离</p>
<p><img src="https://typora-md-bucket.oss-cn-beijing.aliyuncs.com/image-20230928111643057.png" alt="image-20230928111643057"></p>
<p>而对于各类交错分布的情况，可以可以运用聚类方法（例如k-means）将一些类分解成若干个子 类，再用最小距离分类，如下图所示：</p>
<img src="https://typora-md-bucket.oss-cn-beijing.aliyuncs.com/image-20230928111734315.png" alt="image-20230928111734315" style="zoom:67%;" />

<h2 id="3-5-Fisher线性判别"><a href="#3-5-Fisher线性判别" class="headerlink" title="3.5 Fisher线性判别"></a>3.5 Fisher线性判别</h2><h3 id="原理概述"><a href="#原理概述" class="headerlink" title="原理概述"></a>原理概述</h3><p>考虑把d维空间的样本投影到一条直线上，形成一维空间，即把维数压缩到一维，如何根据实际情况找到一条最好的、最易于分类的投影线，这就是Fisher判别方法所要解决的基本问题</p>
<p>为了得到这个最好的、最易于分类的投影线，需要：<strong>使得同类样例的投影点尽可能接近、不同类样例的投影点尽可能远离</strong>如下所示，给出了一个二维示意图：</p>
<img src="https://pic3.zhimg.com/80/v2-e7e2482730034721a2b391b408613d6e_720w.webp" alt="img" style="zoom:67%;" />

<p>上面二维示意图中的”+”、”-“分别代表正例和反例，椭圆表示数据簇的外轮廓，虚线表示投影，红色实心圆和红色实心三角形分别表示两类样本投影后的中心点。</p>
<blockquote>
<p><strong>那么如何理解$w^Tx$?</strong></p>
</blockquote>
<p>答：w在这里可以理解为这条过原点直线的单位方向向量。而 $w^Tx$ ，可以直接利用向量的内积去几何形象地理解它，因此$w^Tx$当做$x$到$w$方向上投影点到原点的距离（原点的一侧为正，另一侧为负）。</p>
<p><img src="https://typora-md-bucket.oss-cn-beijing.aliyuncs.com/image-20231005170242594.png" alt="image-20231005170242594"></p>
<p><img src="https://typora-md-bucket.oss-cn-beijing.aliyuncs.com/image-20230928121049007.png" alt="image-20230928121049007"></p>
<h3 id="Fisher准则函数的定义"><a href="#Fisher准则函数的定义" class="headerlink" title="Fisher准则函数的定义"></a>Fisher准则函数的定义</h3><p>把训练样本集划分为2个子集$X_0$和$X_1$,给出下面的参数：</p>
<ul>
<li><p><font color='red'>各类样本均值向量</font> $:i&#x3D;0,1\ ;\ \ u_{i}&#x3D;{\frac{1}{N_{i}}}\sum_{x\in X_{i}}x$</p>
<p>各类样本均值向量代表某一类(i)中所有样本参数的均值，$w^Tu_i$就是某一类(i)所有样本到原点距离的平均值。</p>
</li>
<li><p><font color='red'>类内离散度矩阵</font> ：$S_{wi}&#x3D;\sum_{x\in X_{i}}\left(x-u_{i}\right)\left(x-u_{i}\right)^{T}$</p>
<p>$\sum_{x\in X_{i}}(w^{T}x-w^{T}u_{i})^{2}$  ，$w^Tx$ 是样本$x$到原点的距离、$w^Tu_i$是类 i 所有样本到原点的平均距离，因此该式是类 i 所有样本到原点距离的方差。现在我们将上面换个矩阵的写法：</p>
<img src="https://typora-md-bucket.oss-cn-beijing.aliyuncs.com/image-20230928123655370.png" alt="image-20230928123655370" style="zoom:67%;" />

<p>因此$w^TS_{bi}w$ 代表到原点距离的方差</p>
</li>
<li><p><font color='red'>总样本类内离散度矩阵</font>：$S_w&#x3D;S_1+S_2$</p>
</li>
<li><p><font color='red'>样本类间散度矩阵</font> ：$S_b &#x3D; (u_1-u_2)(u_1-u_2)^T$</p>
</li>
</ul>
<p>之前提到，Fisher准则函数的目的是：**<font color='red'>在一维Y空间中各类样本尽可能分得开些，即希望两类均值之差越大越好，同时希望各类样本内部尽量密集，即希望类内离散度越小越好</font>**</p>
<ul>
<li><p>首先是希望各类样本内部尽量密集：使得同类样例的投影点之间的距离尽可能接近，只需使得$J_1 &#x3D; w^TS_ww$ 尽可能地小</p>
</li>
<li><p>其次是希望不同样本类样本投影点的距离尽可能地远：只有使得$J_2&#x3D;w^TS_bw$ 尽可能地大</p>
</li>
</ul>
<p>因此可以构造代价函数：<br>$$<br>J&#x3D;\frac{J_{2}}{J_{1}},&#x3D;,\frac{w^{T}S_{b}w}{w^{T}S_{w}w}<br>$$<br>观察这个代价函数，其函数值只与直线的方向$w$有关而与在这个方向上距离是多少无关，因此只需考虑的是$\frac{J_2}{J_1}$ 的比例而无须得到他们的具体值，那么假定$J_1 &#x3D; c \neq 0$  </p>
<p>此时根据拉格朗日常数法求代价函数的导数：</p>
<p><img src="https://typora-md-bucket.oss-cn-beijing.aliyuncs.com/image-20230928130149134.png" alt="image-20230928130149134"></p>
<p><img src="https://typora-md-bucket.oss-cn-beijing.aliyuncs.com/image-20230928130159708.png" alt="image-20230928130159708"></p>
<p>上述过程涉及到矩阵求导，最终得到结果：$w*&#x3D;S_w^{-1}(u_1-u_2)$ ，其中$S_w^{-1}$是$S_w$矩阵的逆矩阵。</p>
<h3 id="多类情况"><a href="#多类情况" class="headerlink" title="多类情况"></a>多类情况</h3><p>前面是针对只有两个类的情况，假设类别变成多个，此时一维已经不能满足要求。假设此时有$C$个类别，需要$K$维（通常设K &#x3D; C - 1）向量来做投影。</p>
<p>K 个投影向量$W&#x3D; [w_1,w_2,…,w_k]$ ，投影后的结果为$y&#x3D;[y_1,y_2,…,y_k]^T$<br>$$<br>y &#x3D; W^Tx<br>$$<br>以下是Fisher线性判别多类情况的基本步骤：</p>
<ol>
<li><p>计算每个类别的<font color='red'>均值向量</font>：对于每个类别，计算其样本的平均值向量，表示为$μ_1、μ_2、…、μ_C$。</p>
</li>
<li><p>计算<font color='red'>类内散布矩阵</font>（within-class scatter matrix）：类内散布矩阵用于度量类别内样本的散布程度。它可以通过计算每个类别内样本与其均值向量之间的差异来获得。对于第i个类别，其类内散布矩阵表示为$S{wi} &#x3D; Σ(x_i - μ_i)(x_i - μ_i)^T$，其中xi是第i个类别的样本，μi是第i个类别的均值向量。</p>
</li>
<li><p>计算<font color='red'>总样本散布矩阵</font>：$S_{w}&#x3D;\sum_{j&#x3D;1}^{C}S_{wj}$</p>
</li>
<li><p>计算<font color='red'>类间散布矩阵</font>（between-class scatter matrix）：类间散布矩阵用于度量不同类别之间的差异性。它可以通过计算不同类别均值向量之间的差异来获得。类间散布矩阵表示为$S_b &#x3D; Σ(Ni)(μ_i - μ)(μ_i - μ)T$，其中Ni是第i个类别的样本数量，μ是所有样本的平均值向量。</p>
</li>
<li><p>计算投影方向：Fisher线性判别的目标是找到一个投影方向，使得投影后的样本在不同类别之间的散布最大化，同一类别内的散布最小化。通过求解广义特征值问题，可以得到w，使得w满足$S_w^{-1}S_b$的最大特征值对应的特征向量。</p>
<p><img src="https://typora-md-bucket.oss-cn-beijing.aliyuncs.com/image-20231001115204000.png" alt="image-20231001115204000"></p>
</li>
</ol>
<h2 id="3-6感知器算法"><a href="#3-6感知器算法" class="headerlink" title="3.6感知器算法"></a>3.6感知器算法</h2><h3 id="感知器算法的由来"><a href="#感知器算法的由来" class="headerlink" title="感知器算法的由来"></a>感知器算法的由来</h3><p>感知器算法（Perceptron Algorithm）是由Frank Rosenblatt于1957年提出的，它是一种基于神经元模型的机器学习算法。Rosenblatt受到生物神经元工作原理的启发，提出了感知器算法作为一种模拟神经元行为的方法。感知器算法在机器学习的发展历程中具有重要的地位，为后续的神经网络和支持向量机等算法奠定了基础。</p>
<h3 id="感知器算法实现二分类"><a href="#感知器算法实现二分类" class="headerlink" title="感知器算法实现二分类"></a>感知器算法实现二分类</h3><p>感知器算法的主要目标是实现二分类。其基本思想是通过一个<strong>线性决策边界</strong>将两个类别分开。算法通过迭代调整权重和阈值，使得对于每个输入样本，如果被正确分类，则权重保持不变，否则进行更新。具体实现步骤如下： </p>
<ol>
<li>初始化权重向量和阈值。</li>
<li>对于每个训练样本，计算输入向量与权重向量的内积，然后与阈值进行比较。</li>
<li>如果内积大于阈值且样本标签为正类，则权重保持不变。</li>
<li>如果内积小于等于阈值且样本标签为负类，则权重向量加上输入向量。</li>
<li>如果内积小于等于阈值且样本标签为正类，则权重向量减去输入向量。</li>
<li>重复上述步骤，直到所有样本都被正确分类或达到预定的迭代次数。</li>
</ol>
<p>感知器算法使用阶跃函数或其他激活函数，将输入向量与权重向量的内积与阈值进行比较，得到最终的分类结果。下面通过一个简单的例子来说明感知器算法的二分类过程：</p>
<p>假设我们有一个二维的数据集，包含两个类别：红色圆圈和蓝色三角形。每个样本都有两个特征：x1和x2。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">红色圆圈（正类）: (1, 2), (2, 3), (2, 1)</span><br><span class="line">蓝色三角形（负类）: (-1, -1), (-2, -3), (-2, -1)</span><br></pre></td></tr></table></figure>

<p>我们的目标是使用感知器算法找到一条直线作为决策边界，将红色圆圈和蓝色三角形分开。</p>
<p>首先，我们初始化权重向量w和阈值b。对于二维数据，权重向量w有两个分量，分别表示x1和x2的权重。我们可以将阈值b看作权重向量的第三个分量。</p>
<p>在开始迭代之前，我们可以随机初始化权重向量和阈值，例如：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">权重向量 w = [0.5, -0.5]</span><br><span class="line">阈值 b = 0</span><br></pre></td></tr></table></figure>

<p>接下来，我们开始迭代处理样本。对于每个样本，我们计算输入向量x与权重向量w的内积，然后与阈值b进行比较。</p>
<p>以第一个样本 (1, 2) 为例：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">内积 = (1 * 0.5) + (2 * -0.5) = 0</span><br></pre></td></tr></table></figure>

<p>由于内积等于阈值，我们判断这个样本被正确分类。</p>
<p>接下来，我们处理第二个样本 (2, 3)：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">内积 = (2 * 0.5) + (3 * -0.5) = 0.5 - 1.5 = -1</span><br></pre></td></tr></table></figure>

<p>由于内积小于阈值，我们判断这个样本被错误分类。根据感知器算法的规则，我们需要更新权重向量和阈值。</p>
<p>更新规则如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">权重向量 w = w + 学习率 * 输入向量</span><br><span class="line">阈值 b = b + 学习率</span><br></pre></td></tr></table></figure>

<p>假设学习率为1，我们可以进行更新：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">权重向量 w = [0.5, -0.5] + [2, 3] = [2.5, 2.5]</span><br><span class="line">阈值 b = 0 + 1 = 1</span><br></pre></td></tr></table></figure>

<p>然后，我们继续处理下一个样本，重复上述步骤。</p>
<p>通过迭代处理所有样本，直到所有样本都被正确分类或达到预定的迭代次数。最终，我们会得到一个能够将红色圆圈和蓝色三角形分开的决策边界。</p>
<p>需要注意的是，感知器算法的收敛性要求数据集是线性可分的。如果数据集不是线性可分的，感知器算法可能无法收敛或产生较</p>
<h3 id="感知器算法实现多分类"><a href="#感知器算法实现多分类" class="headerlink" title="感知器算法实现多分类"></a>感知器算法实现多分类</h3><p>感知器算法最初是为二分类问题设计的，但可以通过一些策略将其扩展为多分类问题的解决方案。</p>
<h4 id="一对多（One-vs-Rest）策略"><a href="#一对多（One-vs-Rest）策略" class="headerlink" title="一对多（One-vs-Rest）策略"></a>一对多（One-vs-Rest）策略</h4><p>在一对多策略中，对于有K个类别的多分类问题，我们训练K个感知器模型。每个模型将一个类别作为正类，而将其他K-1个类别作为负类。在预测阶段，对于一个新的输入样本，每个感知器模型都会产生一个分数，表示样本属于对应类别的概率。最后，选择具有最高概率的类别作为最终的分类结果。</p>
<h4 id="一对一（One-vs-One）策略"><a href="#一对一（One-vs-One）策略" class="headerlink" title="一对一（One-vs-One）策略"></a>一对一（One-vs-One）策略</h4><p>在一对一策略中，对于有K个类别的多分类问题，我们训练K*(K-1)&#x2F;2个感知器模型。每个模型只区分两个类别之间的差异，而忽略其他类别。在预测阶段，每个感知器模型都会对新的输入样本进行分类，最后通过投票或其他策略确定最终的分类结果。</p>
<p>这些方法可以用于将感知器算法扩展到多类分类问题。然而，对于大规模的多类分类问题，这些方法可能会导致模型数量庞大和计算复杂度增加的问题。在实际应用中，可以使用其他更高级的算法，如支持向量机（SVM）或深度学习模型（如神经网络）来处理多类分类问题。</p>
<h3 id="感知器算法的局限性"><a href="#感知器算法的局限性" class="headerlink" title="感知器算法的局限性"></a>感知器算法的局限性</h3><h5 id="非线性可分问题"><a href="#非线性可分问题" class="headerlink" title="非线性可分问题"></a>非线性可分问题</h5><p>感知器算法只适用于线性可分的问题，这意味着存在一个超平面可以将不同类别的样本完全分开。对于非线性可分的问题，感知器算法无法收敛到满足要求的解。</p>
<p>为了解决非线性可分问题，可以使用一些技巧，如引入非线性的特征转换、使用核函数将数据映射到高维空间等。这些技术可以将非线性问题转化为线性可分问题，然后应用感知器算法进行分类。</p>
<hr>
<h5 id="数据噪音"><a href="#数据噪音" class="headerlink" title="数据噪音"></a>数据噪音</h5><p>感知器算法对于包含噪音的数据可能无法收敛，或者产生较差的分类结果。当数据中存在噪音时，感知器算法可能会产生错误的更新，导致无法找到一个合适的决策边界。</p>
<p>为了应对数据噪音，可以采取一些方法来改进感知器算法的鲁棒性，如引入正则化项、增加迭代次数、使用更强大的分类器等。</p>
<hr>
<h5 id="特征缩放和标准化"><a href="#特征缩放和标准化" class="headerlink" title="特征缩放和标准化"></a>特征缩放和标准化</h5><p>感知器算法对输入特征的缩放和标准化敏感。如果输入特征的尺度差异很大，可能会导致算法收敛缓慢或产生不稳定的结果。因此，在应用感知器算法之前，通常需要对输入数据进行预处理，以确保特征具有相似的尺度和分布。</p>
<p>常见的预处理方法包括特征缩放（例如将特征值归一化到0-1范围或使用标准化使其具有零均值和单位方差）和特征标准化（例如将离散特征编码为二进制指示变量）。</p>
<h1 id="第四章、特征选择和提取"><a href="#第四章、特征选择和提取" class="headerlink" title="第四章、特征选择和提取"></a>第四章、特征选择和提取</h1><blockquote>
<p>机器学习在本质上还是特征工程，数据和特征决定了机器学习的上线，而模型和算法只是逼近这个上限而已。																																															—— 吴恩达</p>
</blockquote>
<p>特征选择和提取是构造模式识别系统时的一个关键问题，分类器设计时一直假定已给出了特征向量维数确定的样本集，其中<strong>各样本的每一维都是该样本的一个特征</strong>。特征的选择直接影响到分类器的设计及其性能</p>
<p>例如下面这个例子中：</p>
<p><img src="https://typora-md-bucket.oss-cn-beijing.aliyuncs.com/image-20231024135822708.png" alt="image-20231024135822708"></p>
<p>如果不进行特征选择和提取，会导致数据空间的维度可能非常大， 随着空间维度的增长，数据点越来越分散，以至于距离和密度的概念变得越来越模糊，这会带来维<strong>度灾难</strong>，会导致：</p>
<ol>
<li><strong>分类器性能下降</strong>：当数据点在高维空间中变得分散时，不同类别之间的分界线变得模糊，导致分类器的性能下降。</li>
<li><strong>过拟合</strong>：高维数据中存在的维度灾难可能导致过拟合问题。过拟合指的是模型在训练数据上表现良好，但在新数据上的泛化能力较差。当数据维度较高时，模型可能过于复杂地适应了训练数据中的噪声和随机变化，而无法捕捉到真正的数据模式和规律。</li>
<li><strong>计算复杂性增加</strong>：高维数据的处理和计算复杂性也随之增加。在高维空间中进行特征选择、特征提取和模式识别的计算成本较高，算法的效率和速度可能受到影响。</li>
</ol>
<p><img src="https://typora-md-bucket.oss-cn-beijing.aliyuncs.com/image-20231024140959005.png" alt="image-20231024140959005"></p>
<p>为了设计出效果好的分类器，通常需要对原始的测量值集合进行分析，经过选择或变换处理，组成有效的识别特征；在保证一定分类精度的前提下，减少特征维数，即进行“降维”处理，使分类器实现快速、准确和高效的分类。</p>
<p>因此<strong>特征选择和提取的关键在于</strong>：<font color='red'>所提供的识别特征应具有很好的可分性，使分类器容易判别</font>。</p>
<ul>
<li>因去掉模棱两可、不易判别的特征；</li>
<li>所提供的特征不要重复，即去掉那些相关性强且没有增加更多分类信息的特征。</li>
</ul>
<p>实际上，特征选择和提取这一任务应在设计分类器之前进行：</p>
<p><img src="https://typora-md-bucket.oss-cn-beijing.aliyuncs.com/image-20231024141529135.png" alt="image-20231024141529135"></p>
<p><font color='red'><strong>特征选择</strong></font>：就是从$n$ 个度量值集合${x_1,x_2,…,x_n}$ 中按照<strong>某一准则</strong>选取出供分类使用的子集，作为降维($m$ 维，$m &lt; n$)的分类特征。</p>
<p><font color='red'><strong>特征提取</strong></font>：就是使${x_1,x_2,…,x_n}$ 通过某种变换，产生$m$个特征${y_1,y_2,..,y_m}$ $(m&lt;n)$，作为新的分类特征（或者称为二次特征）；</p>
<p><font color='red'><strong>目的</strong></font>：为了在尽可能<strong>保留识别信息的前提</strong>下，<strong>降低特征空间的维数</strong>，已达到有效的分类。</p>
<p>以细胞自动识别为例：</p>
<ul>
<li><p>通过图像输入得到一批包括正常细胞和异常细胞的图像，我们的任务是根据这些图像区分哪些细胞是正常的，哪些细胞是异常的；</p>
</li>
<li><p>找出一组能代表细胞性质的特征，为此可计算：</p>
<p>细胞总面积、总光密度、胞核面积、核浆比、细胞形状、核内纹理 …</p>
</li>
</ul>
<p>这样产出的<strong>原始特征</strong>会有很多（几十甚至几百个），或者说<strong>原始特征空间维数</strong>很高，需要降低（或称压缩）维数以便分类：</p>
<ul>
<li>一种方式是从原始特征中挑选出一些最有代表性的特征，称之为<strong>特征选择</strong></li>
<li>另一种方式是用映射（或称变换）的方法把原始特征变换为较少的特征，称之为<strong>特征提取</strong> (如Fisher)。</li>
</ul>
<h2 id="特征选择"><a href="#特征选择" class="headerlink" title="特征选择"></a>特征选择</h2><p><img src="https://typora-md-bucket.oss-cn-beijing.aliyuncs.com/image-20231024145454316.png" alt="image-20231024145454316"></p>
<h3 id="手工特征选择"><a href="#手工特征选择" class="headerlink" title="手工特征选择"></a>手工特征选择</h3><p>这是一种简单但经验丰富的方法，通过领域知识和专业经验，人工选择那些认为对于分类任务最具有意义和区分性的特征。手工选择的特征依赖于领域专家的知识和直觉，可以快速实施，但可能无法充分利用数据中的信息。</p>
<p>手工选择会移除下列特征：</p>
<ol>
<li>冗余特征</li>
<li>不相关特征</li>
<li>质量差特征</li>
<li>方差过小的特征</li>
</ol>
<hr>
<h3 id="过滤式选择"><a href="#过滤式选择" class="headerlink" title="过滤式选择"></a>过滤式选择</h3><p>这种方法通过对特征进行评估和排序，选择那些与目标变量相关性较高的特征。常用的过滤方式是设计一个<font color='red'>相关统计量</font>来度量特征的重要性，下面是一些常见的统计量：</p>
<p><img src="https://typora-md-bucket.oss-cn-beijing.aliyuncs.com/image-20231024145955908.png" alt="image-20231024145955908"></p>
<p>过滤式特征选择根据单个特征与目标之间统计分数选择特征，速度快，但缺点是<font color='red'>没有考虑到特征之间的关联作用</font></p>
<hr>
<h3 id="包裹式选择"><a href="#包裹式选择" class="headerlink" title="包裹式选择"></a>包裹式选择</h3><p>这种方法将特征选择看作是一个搜索问题，通过尝<strong>试不同的特征子集</strong>，并使用分类器的性能作为评价指标，选择具有最佳性能的特征子集。常见的包装式选择方法包括递归特征消除（Recursive Feature Elimination，RFE）和遗传算法。包装式选择可以更准确地评估特征子集的性能，但计算复杂度较高</p>
<hr>
<h3 id="嵌入式选择"><a href="#嵌入式选择" class="headerlink" title="嵌入式选择"></a>嵌入式选择</h3><p>这种方法将特征选择与分类器的训练过程融合在一起，通过在分类器训练过程中选择最佳的特征子集。常见的嵌入式选择方法包括L1正则化（如LASSO和岭回归）和决策树算法（如C4.5和Random Forest）。嵌入式选择可以考虑特征之间的相互影响，但计算复杂度较高。</p>
<h2 id="离散K-L变换"><a href="#离散K-L变换" class="headerlink" title="离散K-L变换"></a>离散K-L变换</h2><p>K-L变换全称：Karhunen-Loeve变换（卡洛南-洛伊变换）</p>
<p>特征选择是在一定准则下，删掉某n-k个特征的做法并不十分理想，因为一般来说，原来的n个数据各自在不同程度上反映了识别对象的某些特征，简单地删去某些特征可能会丢失较多的有用信息。</p>
<p>如果将原来的特征做<strong>正交变换</strong>，获得的每个数据都是原来n个数据的线性组合，然后从新的数据中选出少数几个，使其尽可能多地反映各类模式之间的差异，而这些特征间又尽可能相互独立，则比单纯的选择方法更灵活、更有效</p>
<p>K-L变换就是一种适用于<strong>任意概率密度函数</strong>的正交变换。</p>
<h1 id="第六章、有监督学习方法"><a href="#第六章、有监督学习方法" class="headerlink" title="第六章、有监督学习方法"></a>第六章、有监督学习方法</h1></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="http://cjx_0723.gitee.io">Ther</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="http://cjx_0723.gitee.io/2023/10/30/%E5%89%8D%E6%B2%BF%E6%8A%80%E6%9C%AF/AI/%E6%A8%A1%E5%BC%8F%E8%AF%86%E5%88%AB%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">http://cjx_0723.gitee.io/2023/10/30/%E5%89%8D%E6%B2%BF%E6%8A%80%E6%9C%AF/AI/%E6%A8%A1%E5%BC%8F%E8%AF%86%E5%88%AB%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://cjx_0723.gitee.io" target="_blank">Ther的小站</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/AI/">AI</a><a class="post-meta__tags" href="/tags/%E8%AF%BE%E5%A0%82%E5%AD%A6%E4%B9%A0/">课堂学习</a></div><div class="post_share"><div class="social-share" data-image="https://i1.hdslb.com/bfs/face/b6bef67834c33bc7d386fb03a6dbc6db7966102e.jpg@240w_240h_1c_1s_!web-avatar-nav.avif" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2023/10/30/%E5%89%8D%E6%B2%BF%E6%8A%80%E6%9C%AF/%E5%8C%BA%E5%9D%97%E9%93%BE/%E5%8C%BA%E5%9D%97%E9%93%BE%E6%AF%95%E8%AE%BE/" title="区块链毕设"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">区块链毕设</div></div></a></div><div class="next-post pull-right"><a href="/2023/10/30/%E5%89%8D%E6%B2%BF%E6%8A%80%E6%9C%AF/AI/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" title="吴恩达机器学习笔记"><div class="cover" style="background: https://typora-md-bucket.oss-cn-beijing.aliyuncs.com/image-20230925113033196.png"></div><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">吴恩达机器学习笔记</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2023/10/30/%E5%89%8D%E6%B2%BF%E6%8A%80%E6%9C%AF/%E5%A4%A7%E6%A8%A1%E5%9E%8B/%E7%9F%A5%E8%AF%86%E5%B7%A5%E7%A8%8B/" title="知识工程"><div class="cover" style="background: https://w.wallhaven.cc/full/qz/wallhaven-qzom1r.jpg"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-10-30</div><div class="title">知识工程</div></div></a></div><div><a href="/2023/10/30/%E5%89%8D%E6%B2%BF%E6%8A%80%E6%9C%AF/AI/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%AE%AD/" title="吴恩达机器学习实训"><div class="cover" style="background: https://typora-md-bucket.oss-cn-beijing.aliyuncs.com/image-20230925113033196.png"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-10-30</div><div class="title">吴恩达机器学习实训</div></div></a></div><div><a href="/2023/10/30/%E5%89%8D%E6%B2%BF%E6%8A%80%E6%9C%AF/AI/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" title="吴恩达机器学习笔记"><div class="cover" style="background: https://typora-md-bucket.oss-cn-beijing.aliyuncs.com/image-20230925113033196.png"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-10-30</div><div class="title">吴恩达机器学习笔记</div></div></a></div><div><a href="/2023/10/30/%E5%90%8E%E7%AB%AF/%E8%BD%AF%E4%BB%B6%E6%B5%8B%E8%AF%95/%E5%9B%BD%E7%A7%91%E5%A4%A7%E6%B5%8B%E8%AF%95%E8%AF%BE%E7%A8%8B/" title="国科大测试课程"><div class="cover" style="background: https://w.wallhaven.cc/full/1p/wallhaven-1ppld1.jpg"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-10-30</div><div class="title">国科大测试课程</div></div></a></div><div><a href="/2023/10/30/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E5%AD%A6%E7%A7%91/%E8%BD%AF%E4%BB%B6%E5%B7%A5%E7%A8%8B/%E8%BD%AF%E4%BB%B6%E5%88%86%E6%9E%90%E4%B8%8E%E6%B5%8B%E8%AF%95/" title="软件分析与测试"><div class="cover" style="background: https://w.wallhaven.cc/full/yj/wallhaven-yjvxeg.jpg"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-10-30</div><div class="title">软件分析与测试</div></div></a></div><div><a href="/2023/10/30/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E5%AD%A6%E7%A7%91/%E8%BD%AF%E4%BB%B6%E5%B7%A5%E7%A8%8B/%E9%AB%98%E7%BA%A7%E8%BD%AF%E4%BB%B6%E5%B7%A5%E7%A8%8B/" title="高级软件工程"><div class="cover" style="background: https://w.wallhaven.cc/full/qz/wallhaven-qzom1r.jpg"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-10-30</div><div class="title">高级软件工程</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://i1.hdslb.com/bfs/face/b6bef67834c33bc7d386fb03a6dbc6db7966102e.jpg@240w_240h_1c_1s_!web-avatar-nav.avif" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Ther</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">53</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">29</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">8</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%A6%82%E8%BF%B0"><span class="toc-text">概述</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC%E4%BA%8C%E7%AB%A0%E3%80%81%E7%94%9F%E6%88%90%E5%BC%8F%E5%88%86%E7%B1%BB%E5%99%A8"><span class="toc-text">第二章、生成式分类器</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#2-1-%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%A4%E5%88%AB%E5%87%86%E5%88%99"><span class="toc-text">2.1 贝叶斯判别准则</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-2-%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%9C%80%E5%B0%8F%E9%A3%8E%E9%99%A9%E5%88%A4%E5%88%AB"><span class="toc-text">2.2 贝叶斯最小风险判别</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%A3%E4%BB%B7%E7%9F%A9%E9%98%B5"><span class="toc-text">代价矩阵</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B9%B3%E5%9D%87%E6%9D%A1%E4%BB%B6%E9%A3%8E%E9%99%A9"><span class="toc-text">平均条件风险</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BE%8B%E9%A2%98%E8%A7%A3%E6%9E%90"><span class="toc-text">例题解析</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%96%B9%E6%B3%95%E4%B8%80%EF%BC%88%E5%B9%B3%E5%9D%87%E6%9D%A1%E4%BB%B6%E9%A3%8E%E9%99%A9%EF%BC%89%EF%BC%9A"><span class="toc-text">方法一（平均条件风险）：</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%96%B9%E6%B3%95%E4%BA%8C%EF%BC%88%E4%BC%BC%E7%84%B6%E6%AF%94%E4%B8%8E%E9%98%88%E5%80%BC%EF%BC%89%EF%BC%9A"><span class="toc-text">方法二（似然比与阈值）：</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC%E4%B8%89%E7%AB%A0%E3%80%81%E5%88%A4%E5%88%AB%E5%87%BD%E6%95%B0"><span class="toc-text">第三章、判别函数</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#3-1-%E5%88%A4%E5%88%AB%E5%BC%8F%E5%88%86%E7%B1%BB%E5%99%A8%E4%B8%8E%E7%94%9F%E6%88%90%E5%BC%8F%E5%88%86%E7%B1%BB%E5%99%A8"><span class="toc-text">3.1 判别式分类器与生成式分类器</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-2-%E7%BA%BF%E6%80%A7%E5%88%A4%E5%88%AB%E5%87%BD%E6%95%B0"><span class="toc-text">3.2 线性判别函数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-3-%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E5%88%A4%E5%88%AB%E5%87%BD%E6%95%B0"><span class="toc-text">3.3 广义线性判别函数</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A0%B8%E5%BF%83%E6%80%9D%E6%83%B3"><span class="toc-text">核心思想</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8F%8F%E8%BF%B0"><span class="toc-text">描述</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E5%88%A4%E5%88%AB%E5%87%BD%E6%95%B0%E7%9A%84%E6%84%8F%E4%B9%89"><span class="toc-text">广义线性判别函数的意义</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-4-%E5%88%86%E6%AE%B5%E7%BA%BF%E6%80%A7%E5%87%BD%E6%95%B0"><span class="toc-text">3.4 分段线性函数</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%87%BA%E5%8F%91%E7%82%B9"><span class="toc-text">出发点</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%BE%E8%AE%A1"><span class="toc-text">设计</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-5-Fisher%E7%BA%BF%E6%80%A7%E5%88%A4%E5%88%AB"><span class="toc-text">3.5 Fisher线性判别</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8E%9F%E7%90%86%E6%A6%82%E8%BF%B0"><span class="toc-text">原理概述</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Fisher%E5%87%86%E5%88%99%E5%87%BD%E6%95%B0%E7%9A%84%E5%AE%9A%E4%B9%89"><span class="toc-text">Fisher准则函数的定义</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%9A%E7%B1%BB%E6%83%85%E5%86%B5"><span class="toc-text">多类情况</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-6%E6%84%9F%E7%9F%A5%E5%99%A8%E7%AE%97%E6%B3%95"><span class="toc-text">3.6感知器算法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%84%9F%E7%9F%A5%E5%99%A8%E7%AE%97%E6%B3%95%E7%9A%84%E7%94%B1%E6%9D%A5"><span class="toc-text">感知器算法的由来</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%84%9F%E7%9F%A5%E5%99%A8%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B0%E4%BA%8C%E5%88%86%E7%B1%BB"><span class="toc-text">感知器算法实现二分类</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%84%9F%E7%9F%A5%E5%99%A8%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B0%E5%A4%9A%E5%88%86%E7%B1%BB"><span class="toc-text">感知器算法实现多分类</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%80%E5%AF%B9%E5%A4%9A%EF%BC%88One-vs-Rest%EF%BC%89%E7%AD%96%E7%95%A5"><span class="toc-text">一对多（One-vs-Rest）策略</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%80%E5%AF%B9%E4%B8%80%EF%BC%88One-vs-One%EF%BC%89%E7%AD%96%E7%95%A5"><span class="toc-text">一对一（One-vs-One）策略</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%84%9F%E7%9F%A5%E5%99%A8%E7%AE%97%E6%B3%95%E7%9A%84%E5%B1%80%E9%99%90%E6%80%A7"><span class="toc-text">感知器算法的局限性</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E9%9D%9E%E7%BA%BF%E6%80%A7%E5%8F%AF%E5%88%86%E9%97%AE%E9%A2%98"><span class="toc-text">非线性可分问题</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E5%99%AA%E9%9F%B3"><span class="toc-text">数据噪音</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E7%89%B9%E5%BE%81%E7%BC%A9%E6%94%BE%E5%92%8C%E6%A0%87%E5%87%86%E5%8C%96"><span class="toc-text">特征缩放和标准化</span></a></li></ol></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC%E5%9B%9B%E7%AB%A0%E3%80%81%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%E5%92%8C%E6%8F%90%E5%8F%96"><span class="toc-text">第四章、特征选择和提取</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9"><span class="toc-text">特征选择</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%89%8B%E5%B7%A5%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9"><span class="toc-text">手工特征选择</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%BF%87%E6%BB%A4%E5%BC%8F%E9%80%89%E6%8B%A9"><span class="toc-text">过滤式选择</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8C%85%E8%A3%B9%E5%BC%8F%E9%80%89%E6%8B%A9"><span class="toc-text">包裹式选择</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B5%8C%E5%85%A5%E5%BC%8F%E9%80%89%E6%8B%A9"><span class="toc-text">嵌入式选择</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%A6%BB%E6%95%A3K-L%E5%8F%98%E6%8D%A2"><span class="toc-text">离散K-L变换</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC%E5%85%AD%E7%AB%A0%E3%80%81%E6%9C%89%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95"><span class="toc-text">第六章、有监督学习方法</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2023/10/30/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E5%AD%A6%E7%A7%91/%E8%BD%AF%E4%BB%B6%E5%B7%A5%E7%A8%8B/%E9%AB%98%E7%BA%A7%E8%BD%AF%E4%BB%B6%E5%B7%A5%E7%A8%8B/" title="高级软件工程"><div style="background: https://w.wallhaven.cc/full/qz/wallhaven-qzom1r.jpg"></div></a><div class="content"><a class="title" href="/2023/10/30/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E5%AD%A6%E7%A7%91/%E8%BD%AF%E4%BB%B6%E5%B7%A5%E7%A8%8B/%E9%AB%98%E7%BA%A7%E8%BD%AF%E4%BB%B6%E5%B7%A5%E7%A8%8B/" title="高级软件工程">高级软件工程</a><time datetime="2023-10-30T14:04:29.121Z" title="发表于 2023-10-30 22:04:29">2023-10-30</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/10/30/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E5%AD%A6%E7%A7%91/%E8%BD%AF%E4%BB%B6%E5%B7%A5%E7%A8%8B/%E8%BD%AF%E4%BB%B6%E5%88%86%E6%9E%90%E4%B8%8E%E6%B5%8B%E8%AF%95/" title="软件分析与测试"><div style="background: https://w.wallhaven.cc/full/yj/wallhaven-yjvxeg.jpg"></div></a><div class="content"><a class="title" href="/2023/10/30/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E5%AD%A6%E7%A7%91/%E8%BD%AF%E4%BB%B6%E5%B7%A5%E7%A8%8B/%E8%BD%AF%E4%BB%B6%E5%88%86%E6%9E%90%E4%B8%8E%E6%B5%8B%E8%AF%95/" title="软件分析与测试">软件分析与测试</a><time datetime="2023-10-30T14:04:29.118Z" title="发表于 2023-10-30 22:04:29">2023-10-30</time></div></div><div class="aside-list-item"><div class="content"><a class="title" href="/2023/10/30/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E5%AD%A6%E7%A7%91/%E6%95%B0%E5%AD%A6/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1%E7%AC%94%E8%AE%B0/" title="概率论与数理统计">概率论与数理统计</a><time datetime="2023-10-30T14:04:29.068Z" title="发表于 2023-10-30 22:04:29">2023-10-30</time></div></div><div class="aside-list-item"><div class="content"><a class="title" href="/2023/10/30/%E7%94%9F%E6%B4%BB/%E7%A0%94%E7%A9%B6%E7%94%9F%E7%94%9F%E6%B4%BB/PPT/" title="无题">无题</a><time datetime="2023-10-30T14:04:29.061Z" title="发表于 2023-10-30 22:04:29">2023-10-30</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/10/30/%E7%94%9F%E6%B4%BB/%E7%A0%94%E7%A9%B6%E7%94%9F%E7%94%9F%E6%B4%BB/%E7%A0%94%E4%B8%80%E9%80%89%E8%AF%BE/" title="研一选课"><div style="background: https://w.wallhaven.cc/full/4g/wallhaven-4gwl6q.jpg"></div></a><div class="content"><a class="title" href="/2023/10/30/%E7%94%9F%E6%B4%BB/%E7%A0%94%E7%A9%B6%E7%94%9F%E7%94%9F%E6%B4%BB/%E7%A0%94%E4%B8%80%E9%80%89%E8%AF%BE/" title="研一选课">研一选课</a><time datetime="2023-10-30T14:04:29.061Z" title="发表于 2023-10-30 22:04:29">2023-10-30</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2023 By Ther</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, '']
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="algolia-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="search-wrap"><div id="algolia-search-input"></div><hr/><div id="algolia-search-results"><div id="algolia-hits"></div><div id="algolia-pagination"></div><div id="algolia-info"><div class="algolia-stats"></div><div class="algolia-poweredBy"></div></div></div></div></div><div id="search-mask"></div><script src="https://cdn.jsdelivr.net/npm/algoliasearch/dist/algoliasearch-lite.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instantsearch.js/dist/instantsearch.production.min.js"></script><script src="/js/search/algolia.js"></script></div></div></body></html>